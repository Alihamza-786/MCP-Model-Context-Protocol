{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b009b998-62a7-4185-b65f-0f8885bd3f0c",
   "metadata": {},
   "source": [
    "cat > server_config.json << EOF\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \".\"\n",
    "            ]\n",
    "        },\n",
    "\n",
    "        \"research\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\"run\", \"research_server.py\"]\n",
    "        },\n",
    "        \n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\"mcp-server-fetch\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa75cdb5-665c-4243-ada3-88dc18437d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict, Any\n",
    "from contextlib import AsyncExitStack\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    type: str\n",
    "    function: Dict[str, Any]\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.client = OpenAI()\n",
    "        self.available_tools: List[ToolDefinition] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\n‚úÖ Connected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for tool in tools:\n",
    "                # Map tool name to the session so we can call it later\n",
    "                self.tool_to_session[tool.name] = session\n",
    "\n",
    "                # Register with OpenAI, including the required 'type' and 'function' wrapper\n",
    "                self.available_tools.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"parameters\": tool.inputSchema\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open('server_config.json', 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            servers = data.get('mcpServers', {})\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def process_query(self, query: str):\n",
    "        messages: List[Dict[str, Any]] = [\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        process_query = True\n",
    "\n",
    "        # Kick off the first OpenAI call\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=self.available_tools,\n",
    "            max_tokens=2024\n",
    "        )\n",
    "\n",
    "        while process_query:\n",
    "            choice = response.choices[0]\n",
    "            msg = choice.message\n",
    "            tool_calls = getattr(msg, \"tool_calls\", None)\n",
    "\n",
    "            if tool_calls:\n",
    "                # There may be one or more tool calls in this choice\n",
    "                for tc in tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    raw_args = tc.function.arguments\n",
    "\n",
    "                    # Parse into a Python dict if it's a JSON string\n",
    "                    if isinstance(raw_args, str):\n",
    "                        tool_args = json.loads(raw_args)\n",
    "                    else:\n",
    "                        tool_args = raw_args\n",
    "\n",
    "                    print(f\"\\nüîß Calling tool `{tool_name}` with args: {tool_args}\")\n",
    "                    session = self.tool_to_session[tool_name]\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "\n",
    "                    # Record that we invoked the tool\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": None,\n",
    "                        \"tool_calls\": [tc]\n",
    "                    })\n",
    "                    # Append the tool result\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"content\": result.content\n",
    "                    })\n",
    "\n",
    "                    # Ask OpenAI for the next step\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=messages,\n",
    "                        tools=self.available_tools,\n",
    "                        max_tokens=2024\n",
    "                    )\n",
    "            else:\n",
    "                # No more tools to call ‚Äî print the final assistant message\n",
    "                print(f\"\\nü§ñ Assistant: {msg.content}\")\n",
    "                process_query = False\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print('\\nü§ñ MCP Chatbot Started!')\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nüí¨ Query: \").strip()\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                await self.process_query(query)\n",
    "                print('\\n')\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        await chatbot.connect_to_servers()\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76850951-146c-4f75-bc53-d2c4f1f2d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"768\"\n",
       "            src=\"http://localhost:8888/terminals/3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7c7a4d5a8860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new terminal\n",
    "import os\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"http://localhost:8888/terminals/3\", width=600, height=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52267afb-5f28-446b-b0a1-36913791f129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
